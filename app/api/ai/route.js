import { NextResponse } from "next/server";
import { GoogleGenAI } from "@google/genai";

// Initialize the Google GenAI client with the API key from environment variables.
const ai = new GoogleGenAI({ apiKey: process.env.api });

export async function POST(request) {
  try {
    const body = await request.json();

    // The client should send the data as Base64 encoded strings,
    // so we don't need a separate conversion step here.
    const cctBase64 = body.img;
    const pdfBase64 = body.pdf;

    const prompt = `"I have two files that need analysis:
      CCTV Image: A surveillance snapshot showing a specific location
      Typhoon Weather Report: Meteorological data for an incoming typhoon affecting this area
      Analysis Requirements:
      Examine the CCTV image to identify:
      Building types, construction materials, and structural condition
      Elevation and topography of the area
      Infrastructure present (roads, power lines, drainage, etc.)
      Vegetation and natural features
      Population density indicators
      Output Needed:
      Create a clear, descriptive risk assessment report that includes:
      Location Description: Detailed analysis of what's visible in the CCTV image
      Vulnerability Assessment: How the observed structures and terrain will respond to the typhoon conditions
      Risk Levels: Categorized threat levels for different types of damage
      Public Recommendations: Clear, actionable steps for residents in this area
      Target Audience: General public who live in or near this location
      Tone: Professional but easily understandable, avoiding technical jargon
      Format: Structured report with clear sections and bullet points for quick reference`;
      
    if (!cctBase64 || !pdfBase64) {
      return NextResponse.json(
        {
          error:
            "Please provide both an image (img) and a PDF (pdf) file in the request body.",
        },
        { status: 400 }
      );
    }

    // Construct the contents array for the multimodal request.
    // Each piece of data (image, pdf, text) is a separate part in a single parts array.
    const contents = [
      {
        parts: [
          // Part for the CCTV image
          {
            inlineData: {
              mimeType: "image/jpeg", // Assuming JPEG for the CCTV image. Adjust if needed.
              data: cctBase64,
            },
          },
          // Part for the PDF file
          {
            inlineData: {
              mimeType: "application/pdf",
              data: pdfBase64,
            },
          },
          // Part for the text prompt
          {
            text: prompt,
          },
        ],
      },
    ];

    // Call the Gemini API to generate content based on the multimodal input.
    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash",
      contents: contents,
    });

    // Extract the generated text from the API response.
    const generatedText = response?.candidates?.[0]?.content?.parts?.[0]?.text;

    // Check if a response was successfully generated.
    if (!generatedText) {
      return NextResponse.json(
        {
          error:
            "No text was generated by the API. The prompt or data might be insufficient.",
        },
        { status: 500 }
      );
    }

    // Return the generated text in the response.
    return NextResponse.json({
      text: generatedText,
      success: true,
    });
  } catch (error) {
    console.error("Error in API route:", error);

    return NextResponse.json(
      {
        error: "An internal server error occurred.",
        details: error.message,
      },
      { status: 500 }
    );
  }
}
