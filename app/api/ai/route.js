import { NextResponse } from "next/server";
import { GoogleGenAI } from "@google/genai";

// Initialize the Google GenAI client with the API key from environment variables.
const ai = new GoogleGenAI({ apiKey: process.env.api });

export async function POST(request) {
  try {
    const body = await request.json();

    // The client should send the data as Base64 encoded strings,
    // so we don't need a separate conversion step here.
    const cctBase64 = body.img;
    const pdfBase64 = body.pdf;

    const prompt =
      "There is this two file one is image its a cctv snapshot as you can see its a location analyze its all what you can see and the other file is a typhoon report that is incoming to the cctv snapshot i want you to assess what are the possbile damages or the possible result of this weather report and with that make a report that is easily comprehensive because a mass of people will read it and use it as a reference for what they should do like the poeple live near it";

    // Validate that both image and PDF data are present.
    if (!cctBase64 || !pdfBase64) {
      return NextResponse.json(
        {
          error:
            "Please provide both an image (img) and a PDF (pdf) file in the request body.",
        },
        { status: 400 }
      );
    }

    // Construct the contents array for the multimodal request.
    // Each piece of data (image, pdf, text) is a separate part in a single parts array.
    const contents = [
      {
        parts: [
          // Part for the CCTV image
          {
            inlineData: {
              mimeType: "image/jpeg", // Assuming JPEG for the CCTV image. Adjust if needed.
              data: cctBase64,
            },
          },
          // Part for the PDF file
          {
            inlineData: {
              mimeType: "application/pdf",
              data: pdfBase64,
            },
          },
          // Part for the text prompt
          {
            text: prompt,
          },
        ],
      },
    ];

    // Call the Gemini API to generate content based on the multimodal input.
    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash",
      contents: contents,
    });

    // Extract the generated text from the API response.
    const generatedText = response?.candidates?.[0]?.content?.parts?.[0]?.text;

    // Check if a response was successfully generated.
    if (!generatedText) {
      return NextResponse.json(
        {
          error:
            "No text was generated by the API. The prompt or data might be insufficient.",
        },
        { status: 500 }
      );
    }

    // Return the generated text in the response.
    return NextResponse.json({
      text: generatedText,
      success: true,
    });
  } catch (error) {
    console.error("Error in API route:", error);

    return NextResponse.json(
      {
        error: "An internal server error occurred.",
        details: error.message,
      },
      { status: 500 }
    );
  }
}
